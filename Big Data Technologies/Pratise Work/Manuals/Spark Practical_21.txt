1.Creating and displaying an RDD
……………………………………..................................................................................................................
  words = sc.parallelize (["scala",  "java",  "hadoop",  "spark",  "akka",  "spark vs hadoop", "pyspark",  "pyspark and spark"])
  New_RDD.take(3)   // Take method is used to retrive data from the RDD
…………………………………………………………………………....................................................................................................
2.Reading data from a text file and displaying the first 4 elements
  New_RDD = sc.textFile("file:/home/cloudera/Desktop/detail.csv")
  New_RDD.collect() //collect() method on RDD returns list of all the elements of the RDD. It’s a great asset for displaying all the contents of our RDD
  or        print(New_RDD.collect())  // using print method
  New_RDD.take(4)  // to fetch few item from the RDD
...................................................................................................................................
3.Changing minimum number of partitions and mapping the data from CSV file
...................................................................................................................................
CSV_RDD = (sc.textFile("file:/home/cloudera/Desktop/detail.csv", minPartitions= 4).map(lambda element: element.split("\t"))) 
CSV_RDD.take(3)
.................................................................................................................................
Basic PySpark RDD operations : count(), first(),take(),collect(), reduce(),.saveAsTextFile(), reduceByKey(),sortByKey()
4. Operations on RDD of CSV file
   A. CSV_RDD.count()                 // returns the number of elements of our RDD

   B. first()   
      CSV_RDD.first()                         // it will return the first element from the RDD

   C. take(argument) //The .take(n) -n number of elements retrived from the RDD. 

      CSV_RDD.take(4)
   D. New_RDD.collect() //collect() method on RDD returns list of all the elements of the RDD.

   E. reduce() // The .reduce() Action takes two elements from the given RDD and operates on these elements as per expression.operation is performed using   
                                 an anonymous function or lambda
      a_rdd = sc.parallelize([1,3,4,8,6])         // create a RDD
      print(a_rdd.reduce(lambda x, y : x /y))
      print(a_rdd.reduce(lambda x, y : x +y))         // output 22
      print(a_rdd.reduce(lambda x, y : x *y))           // output 576
      print(a_rdd.reduce(lambda x, y : x -y))            // output -20 

   F. max() – Returns max record from RDD
      a_rdd = sc.parallelize([1,3,4,8,6]) 
      datMax = a_rdd.max()
      print("Max Record :",datMax)

  G.  The .saveAsTextFile()  // create a RDD and save it to HDFS directory
      s_rdd = sc.parallelize([1,2,3,4,5,6]) 
      print(s_rdd.collect())
      s_rdd.saveAsTextFile('/data.txt') // by default will save at HDFS
      ******************Working with Key-Value Pairs
 H.  marks_rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), ('Shreya', 28),   
                ('Abhay', 26), ('Rohan', 22)])
     print(marks_rdd.reduceByKey(lambda x, y: str(x + y)).collect())  
     //performs multiple parallel processes for each key in the data and combines the values for the same keys
     output ***************[('Abhay', '55'), ('Rohan', '44'), ('Swati', '45'), ('Shreya', '50'), ('Rahul', '48')]

 I. The .sortByKey() transformation sorts the input data by keys from key-value pairs in ascending order.
      By default, sortByKey() sorts elements in ascending order.
      marks_rdd = sc.parallelize([('Rahul', 55), ('Swati', 76), ('Shreya', 52), ('Abhay', 19), ('Rohan',32)])
      print(marks_rdd.sortByKey('ascending').collect()) // output : [('Abhay', 19), ('Rahul', 55), ('Rohan', 32), ('Shreya', 52), ('Swati', 76)]
........................................................................................................
9. Creating RDDs with key-value pair
a = sc.parallelize([('a',50),('b',70)]) 
b = sc.parallelize([('a',97),('b',57),('c',20)])
c = a.join(b)   // join operation
c.collect()
.........................................................................................................
10. Performing Set Operations
  
A.intersection-
  rdd_a = sc.parallelize([1,2,3,4])              ##Create two new RDDs 
  rdd_b = sc.parallelize([3,4,5,6])
  rdd_a.intersection(rdd_b).collect()
B.Subtraction
  rdd_a.subtract(rdd_b).collect()
C.Cartesian
  rdd_a.cartesian(rdd_b).collect()
D. Union
 rdd_a.union(rdd_b).collect()

----------------------------------------------------------------------------
@@@  filter() and map() groupByKey()
  J.  Creating a new RDD with flattened data and filtering out the ‘stopwords’ from the entire RDD
      stopwords = ['a','all','the','as','is','am','an','and','be','been','from','had','I','I’d','why','with']
      RDD1 = stopwords.filter(lambda x: x not in stopwords)
      RDD1.take(4)
      Filtering the words starting with ‘b’ // choose any alphabet
      filteredRDD = RDD.filter(lambda x: x.startswith('b'))
      filteredRDD.distinct().take(50)

....................................................................
Pyspark Dataframes Example 1: FIFA World Cup Dataset
...................................................................
Step1-Let’s load the data from a CSV file by using the method spark.read.format[csv/json].
diab_df = spark.read.csv("file:///C:/Users/CDAC/Desktop/diabt.csv", inferSchema = True, header = True) 
diab_df.show()
.....................
Schema of Dataframe- printSchema method
fifa_df.printSchema()
-------------------------
Column Names and Count (Rows and Column)-When we want to have a look at the names and a count of the number of 
Rows and Columns of a particular Dataframe, we use the following methods.
diab_df.columns //Column Names 
diab_df.count() // Row Count 
len(fifa_df.columns) //Column Count
...................................................
Describing a Particular Column -If we want to have a look at the summary of any particular column of a Dataframe, we use the describe method
diab_df.describe('Age').show() 

.................................................................
Selecting Multiple Columns -select particular columns from the dataframe, we use the select method.
diab_df.select('col1','Col2','col3').show()
diab_df.select('Outcome','Age','BMI').show()
.....................................................................................................
Selecting Distinct Multiple Columns by using Disctinct method
fifa_df.select('Player Name','Coach Name').distinct().show()
...................................................................................................
Filtering Data -In order to filter the data, according to the condition specified, we use the filter command.
diab_df.filter(diab_df.MatchID=='1096').show() 
#diab_df.filter(diab_df.Age=='31').show()
to count - diab_df.filter(fifa_df.Age=='1096').count()
...............................................................................................................
Filtering Data (Multiple Parameters)-We can filter our data based on multiple conditions (AND or OR)
diab_df.filter((diab_df.Age=='31') & (diab_df.Outcome=='1')).show()
...........................................................................................................
Sorting data - Sorting Data (OrderBy)- 
diab_df.orderBy(diab_df.Age).show()