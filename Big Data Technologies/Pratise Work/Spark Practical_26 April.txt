1.Creating and displaying an RDD
……………………………………..................................................................................................................
  words = sc.parallelize (["scala",  "java",  "hadoop",  "spark",  "akka",  "spark vs hadoop", "pyspark",  "pyspark and spark"])
…………………………………………………………………………....................................................................................................
2.Reading data from a text file and displaying the first 4 elements
  New_RDD = sc.textFile("file:///C:/Users/CDAC/Desktop/test.txt")
  New_RDD.take(4)
...................................................................................................................................
3.Changing minimum number of partitions and mapping the data from CSV file
...................................................................................................................................
CSV_RDD = (sc.textFile("file:///C:/Users/CDAC/Desktop/diabt.csv", minPartitions= 4).map(lambda element: element.split("\t"))) 
CSV_RDD.take(3)
.................................................................................................................................
4. Operations on RDD of CSV file
   CSV_RDD.count()
........................................................................................
6. Creating a new RDD with flattened data and filtering out the ‘stopwords’ from the entire RDD
 stopwords = ['a','all','the','as','is','am','an','and','be','been','from','had','I','I’d','why','with']
 RDD = New_RDD.flatMap(Func)
 RDD1 = RDD.filter(lambda x: x not in stopwords)
 RDD1.take(4)
....................................s..............................................................
7.Filtering the words starting with ‘b’ // choose any alphabet
import re
filteredRDD = RDD.filter(lambda x: x.startswith('b'))
filteredRDD.distinct().take(50)
.........................................................................................................
8.Grouping the data by key and then sorting it
rdd_mapped = RDD.map(lambda x: (x,1))
rdd_grouped = rdd_mapped.groupByKey()
rdd_frequency = rdd_grouped.mapValues(sum).map(lambda x: (x[1],x[0])).sortByKey(False)
rdd_frequency.take(10)
........................................................................................................
9. Creating RDDs with key-value pair
a = sc.parallelize([('a',50),('b',70)]) 
b = sc.parallelize([('a',97),('b',57),('c',20)])
c = a.join(b) 
c.collect()
.........................................................................................................
10. Saving the data in a text file
RDD3.saveAsTextFile("file:///C:/Users/CDAC/Desktop/newoutput.txt")
..........................................................................................................
11. Performing Set Operations
##Create two new RDDs 
A.intersection-
rdd_a = sc.parallelize([1,2,3,4]) 
rdd_b = sc.parallelize([3,4,5,6])
rdd_a.intersection(rdd_b).collect()
B.Subtraction
rdd_a.subtract(rdd_b).collect()
C.Cartesian
rdd_a.cartesian(rdd_b).collect()
D. Union
rdd_a.union(rdd_b).collect()
....................................................................
Pyspark Dataframes Example 1: FIFA World Cup Dataset
...................................................................
Step1-Let’s load the data from a CSV file by using the method spark.read.format[csv/json].
diab_df = spark.read.csv("file:///C:/Users/CDAC/Desktop/diabt.csv", inferSchema = True, header = True) 
diab_df.show()
.....................
Schema of Dataframe- printSchema method
-fifa_df.printSchema()
-------------------------
Column Names and Count (Rows and Column)-When we want to have a look at the names and a count of the number of 
Rows and Columns of a particular Dataframe, we use the following methods.
diab_df.columns //Column Names 
diab_df.count() // Row Count 
len(fifa_df.columns) //Column Count
...................................................
Describing a Particular Column -If we want to have a look at the summary of any particular column of a Dataframe, we use the describe method
diab_df.describe('Age').show() 

.................................................................
Selecting Multiple Columns -select particular columns from the dataframe, we use the select method.
diab_df.select('col1','Col2','col3').show()
diab_df.select('Outcome','Age','BMI').show()
.....................................................................................................
Selecting Distinct Multiple Columns by using Disctinct method
fifa_df.select('Player Name','Coach Name').distinct().show()
...................................................................................................
Filtering Data -In order to filter the data, according to the condition specified, we use the filter command.
diab_df.filter(diab_df.MatchID=='1096').show() 
#diab_df.filter(diab_df.Age=='31').show()
to count - diab_df.filter(fifa_df.Age=='1096').count()
...............................................................................................................
Filtering Data (Multiple Parameters)-We can filter our data based on multiple conditions (AND or OR)
diab_df.filter((diab_df.Age=='31') & (diab_df.Outcome=='1')).show()
...........................................................................................................
Sorting data - Sorting Data (OrderBy)- 
diab_df.orderBy(diab_df.Age).show()